---
title: "US Foreign Policy"
subtitle: "Research Methods Primer"
author: 
  - Michael Flynn
institute:
  - Professor
  - Department of Political Science
  - 011C Calvin Hall
  - meflynn@ksu.edu
date: last-modified
resources: 
  - ."../images/"
  - "images/"
format:
  revealjs:
    theme: [quarto-lecture-theme.scss, default]
    #css: quarto-lecture-theme.css
    auto-stretch: false # Needed to stop figures from stretching and ignoring figure size
    slide-number: true
    chalkboard: true
    height: 900
    width: 1600
    #incremental: true
title-slide-attributes:
    data-background-image: "ksu-seal.png"
    data-background-size: 45% 
    data-background-position: bottom -38% right -19%
    #data-background-size: 750px 750px
    #data-background-position: 1000px 370px
    #data-background-position: 115% 150%
    #data-background-position: 115% 140%
    #data-background-position: 19em 8em
    #height: 700
    #width: 1050
---
```{r setup}
#devtools::install_github("meflynn/flynnprojects")
#devtools::install_github("MatthewBJane/theme_park")
library(tidyverse)
library(ggdag)
library(dagitty)
library(flynnprojects)
library(ThemePark)

sysfonts::font_add_google("Oswald", "oswald")
showtext::showtext_auto()
showtext::showtext_opts(dpi = 300)
```




##  Overview

What do we want to get out of today's class?

1. Learn how to read a research paper
2. An understanding of some of the general methods we use in research
3. Learn the basics of how to interpret statistical models
4. Some of the different types of bias and ways that data can fool us
5. Learn the importance of theory


# Reading Research {.inverse}

## The Anatomy of a Research Paper

::::{.columns}
:::{.column}
How you'll want to read it:

1. Abstract
2. Introduction
3. Theory
4. Research Design
5. Results
6. Conclusion
:::

:::{.column}

:::
::::


## The Anatomy of a Research Paper

::::{.columns}
:::{.column}
How you'll want to read it:

1. Abstract
2. Introduction
3. Theory
4. Research Design
5. Results
6. Conclusion
:::

:::{.column}
How you _should_ read it (as a rough guide):

1. Abstract
2. Introduction
3. Conclusion
4. Results
5. Research Design
6. Theory
:::
::::


# Methods {.inverse}

## Common Methods

We use lots of different methods in the social sciences, but let's look at a few key approaches/tools:

- Statistical modeling and regression analysis
- Formal (mathematical) modeling
- Experimental methods
- Case studies
- Field work
- Surveys
- Archival research


:::{.notes}
Remember our goals:
- Understand the world
- Understand causal effects
- Make better predictions
- Maybe use this information to make better decisions/politics

Buuuut...people are bad at assessing lots of complex information. 

We can adopt different strategies to do this:

- First we read a lot of substantive material and history to understand what the issues are
- Next we need to figure out what the most basic and important moving parts are.
:::


# Interpretation {.inverse}


## Interpreting and Assessing Statistical Models

First, what are our goals?

- Prediction?
- Description?
- Estimating causal effects?




## Interpreting and Assessing Statistical Models

::::{.columns}
:::{.column width="33%"}
**Prediction**

- Overall model fit

- In-sample fit?

- Out-of-sample fit?
:::

:::{.column width="33%"}
**Description**

- Overall model fit

- Individual coefficients

- Associations between groups

:::

:::{.column width="33%"}
**Causal Inference**

- Focus on individual coefficients for causal effects
:::

::::



## Interpretting and Assessing Statistical Models

What is a variable?

- A variable is a characteristic, trait, attribute, or condition that can be measured or observed

- A "definable quantity that can take on two or more values" (Kellstedt and Whitten 2018, 22).

- Examples:
  - Age
  - Income
  - Education
  - Speed of a moving vehicle
  - Color a car is painted
  - Whether a person is a Democrat or Republican
  - If a member of Congress served in the military or not





## Interpreting and Assessing Statistical Models

What is a coefficient?

\begin{equation} 
Y_i = \alpha + {\color{blue} \beta_1} X_i + {\color{blue}\beta_2} Z_i + \epsilon_i 
\end{equation}

- The correlation between $X$ and $Y$, conditional on other predictors and model

- Typically denotes the magnitude of the change in $Y$ that corresponds to a one-unit change in $X$

- In some cases this is direct and easy, less so in other cases

- Different models and measurement schemes mean we need to be cautious in interpreting coefficients



## Interpreting and Assessing Statistical Models

![Example of a regression table from a research article](../images/regression-table-screenshot.jpg)


# Sources of Bias {.inverse}

## What's "Bias"?

  
> **Bias:** Systematic error of some sort that can affect our inferences and conclusions (Howard 2017, 151)
  
- What it's not:

  - Presenting "one side"
  - "Bias" in the colloquial cable news meaning of "Anything I don't like" or "Unflattering coverage of people and things I do like."




## Confounding

::::{.columns}
:::{.column}
What is confounding?

- Confounding is when the predictor (i.e. X) and the outcome (i.e. Y) share a common ancestor (i.e. Z).

- If we don't take into account the relationship between Z and the other variables, our estimates of X are biased.

- The most common way to do this is to include Z into our model

\begin{align}
Y & = \alpha + \beta_1 X_i + \epsilon_i \\
vs \\
Y & = \alpha + \beta_1 X_i + \beta_2 Z_i + \epsilon_i
\end{align}
:::

:::{.column}
```{r confounding-example}
#| echo: false
#| dpi: 300
#| out-width: "90%"

library(dagitty)
library(ggdag)
library(tidyverse)
library(viridis)

dag.df <- dagitty(' dag{
                  "x" [exposure]
                  "y" [outcome]
                  
                  "x" [pos="1,1"]
                  "y" [pos="2,1"]
                  "z" [pos="1.5,2"]
                  
                  "x" -> "y"
                  "y" <- "z" -> "x"
}') %>% 
  tidy_dagitty()



ggplot(dag.df, aes(x = x, xend = xend, y = y, yend = yend)) +
  geom_dag_point(aes(color = name), show.legend = FALSE, size = 20) +
  geom_dag_edges(size = 10) +  
  geom_text(aes(label = name), color = "white", size = 10, parse = TRUE) +
  theme_dag() +
  scale_color_viridis(discrete = TRUE, begin = 0.1, end = 0.9) 

```
:::
::::


## Confounding Example

::::{.columns}
:::{.column}


Let's imagine we're interested in the relationship between gas pedal pressure and speed, so we collect data on

- A few hundred drivers
- Their speed in mph
- How hard they press the pedal
```{r}
N <- 1e3
#"0°", "5°", "10°", "15°", "20°"
cardata <- data.frame(incline = runif(n = N, min = 0, max = 45)) |> 
  mutate(pedal_pressure = 20 + incline * 1.2 + rnorm(n = N, mean = 0, sd = 0.4),
         speed = 45 + 1.25*pedal_pressure - 1.50*incline + rnorm(n = N, mean = 0, sd = 0.2)) 

lm(speed ~ pedal_pressure, data = cardata)
```
:::

:::{.column}
```{r confounding-car-1, fig.pos="center"}
#| out.width: "88%"

ggplot(cardata, aes(x = pedal_pressure, y = speed)) +
  geom_point(size = 3.5, shape = 21, fill = "black") +
  theme_flynn(base_family = "oswald") +
  scale_x_continuous(breaks = seq(20, 100, 10)) +
  viridis::scale_fill_viridis(option = "magma") +
  labs(x = "Pedal Pressure",
       y = "Speed (mph)",
       title = "Car speed and pedal pressure",
       subtitle = "We should expect cars to go faster as we apply more pressure, so what gives?")

```
:::
::::

## Confounding Example

::::{.columns}
:::{.column}
Confounding can mask causal relationships

- We need to understand the data generating process to help us make sense of results that may or may not themselves make sense

- Adjusting for confounders in our model can reduce bias and help us better estimate causal relationships
```{r}
lm(speed ~ pedal_pressure + incline, data = cardata)
```

:::

:::{.column}
```{r confounding-car-2, fig.pos="center"}
#| out.width: "88%"

ggplot(cardata, aes(x = pedal_pressure, y = speed, fill = incline)) +
  geom_point(size = 3.5, shape = 21) +
  theme_flynn(base_family = "oswald") +
  scale_x_continuous(breaks = seq(20, 100, 10)) +
  viridis::scale_fill_viridis(option = "magma") +
  labs(x = "Pedal Pressure",
       y = "Speed (mph)",
       fill = "Incline",
       title = "Car speed and pedal pressure",
       subtitle = "We should expect cars to go faster as we apply more pressure, so what gives?")

```
:::
::::


## Base Rate Fallacy

::::{.columns}
:::{.column}
What is it and why does it matter?

- Counts can be deceptive without information about population of interest
- Example of vaccine status and hospitalization
- Vaccines are effective at reducing risk of severe illness, hospitalization, etc.
- And yet in some locations we see more hospital beds occupied by vaccinated people than non-vaccinated people
- Huh? What gives?
```{r base-rate-fallacy}
#| out.width: "88%"

set.seed(12)

N.1 <- 1e3
N.2 <- 1e3

mu.1 <- c(X  = -5, Y = 0)
sigma.1 <- matrix(c(1, 0, 0, 1), ncol = 2)

mu.2 <- c(X  = 5, Y = 0)
sigma.2 <- matrix(c(1, 0, 0, 1), ncol = 2)


baserate <- data.frame("Vaccinated" = MASS::mvrnorm(n = N.1, mu = mu.1, Sigma = sigma.1),
                       "Unvaccinated" = MASS::mvrnorm(n = N.2, mu = mu.2, Sigma = sigma.2)) |> 
  pivot_longer(cols = 1:4,
               names_to = c("Group", "Var"),
               names_sep = c("\\.")) |> 
  pivot_wider(names_from = "Var") |> 
  unnest() |> 
  slice(-c(2000:1300)) |> 
  group_by(Group) |> 
  mutate(Hospitalized = case_when(
    Group == "Vaccinated" ~ rbinom(n = length(.data$Group=="Vaccinated"), size = 1, prob = 0.05),
    Group == "Unvaccinated" ~ rbinom(n = length(.data$Group=="Unvaccinated"), size = 1, prob = 0.10)
  ),
  Hospitalized = factor(Hospitalized,
                        levels = c(0, 1),
                        labels = c("Not Hospitalized", "Hospitalized")),
  Group = factor(Group, 
                 levels = c("Vaccinated" , "Unvaccinated"),
                 labels = c("Vaccinated", "Unvaccinated")))
  
counts <- data.frame(status = c("Vaccinated", "Unvaccinated"),
                     count =c(sum(baserate$Hospitalized=="Hospitalized" & baserate$Group=="Vaccinated"),
                     sum(baserate$Hospitalized=="Hospitalized" & baserate$Group=="Unvaccinated")))
```
:::

:::{.column}

```{r base-rate-fallacy-figure, fig.pos="center"}
#| out.width: "88%"

library(gridExtra)
library(ggpp)

ggplot(baserate |> filter(Hospitalized == "Hospitalized"), aes(x = Group, y = Y, fill = Group)) +
  geom_point(size = 2.5, shape = 21, position = position_jitter(width = 0.2)) +
  geom_table(data = counts, aes(x = 0.5, y = 2.5), label = list(counts)) +
  theme_flynn() +
    theme(text = element_text(family = "oswald"),
          axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  viridis::scale_fill_viridis(option = "magma", 
                              discrete = TRUE,
                              begin = 0.0,
                              end = 0.9) +
  labs(x = "",
       y = "",
       title = "Hospitalization of Vaccinated and Unvaccinated Individuals",
       subtitle = "How can the vaccine be protective if most people in the hospital are vaccinated?")


```
:::
::::


## Base Rate Fallacy

::::{.columns}
:::{.column}
The underlying population matters!

- The **rate** of an event may vary across groups

- Knowing something about the reference population is key

- Another way to think of this is the symmetry/asymmetry of conditional probabilities

$$Pr(H \mid V) \neq Pr(V \mid H)$$

:::

:::{.column}
```{r base-rate-fallacy-2, fig.pos="center"}
#| out.width: "88%"

counts2 <- data.frame(status = c("Vaccinated", "Unvaccinated"),
                     count = c(sum(baserate$Hospitalized=="Hospitalized" & baserate$Group=="Vaccinated"),
                     sum(baserate$Hospitalized=="Hospitalized" & baserate$Group=="Unvaccinated")),
                     total = c(sum(baserate$Group=="Vaccinated"), sum(baserate$Group=="Unvaccinated"))) |> 
  mutate(percent = (count/total)*100)

ggplot() +
  geom_point(data = baserate |> filter(Hospitalized == "Not Hospitalized"), 
             aes(x = Group, y = Y, fill = Hospitalized), 
             size = 2.5, 
             shape = 21, 
             alpha = 0.6,
             position = position_jitter(width = 0.2)) +
  geom_point(data = baserate |> filter(Hospitalized == "Hospitalized"), 
             aes(x = Group, y = Y, fill = Hospitalized), 
             size = 2.5, 
             shape = 21,
             position = position_jitter(width = 0.2)) +
  geom_table(data = counts2, aes(x = 0.5, y = 2.5), label = list(counts2)) +
  theme_flynn() +
  viridis::scale_fill_viridis(option = "magma", 
                              discrete = TRUE,
                              begin = 0.0,
                              end = 0.9) +
  theme(text = element_text(family = "oswald"),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  labs(x = "",
       y = "",
       title = "Hospitalization Among Vaccinated and Unvaccinated Individuals",
       subtitle = "How can the vaccine be protective if most hospitalized people are vaccinated?")


```
:::
::::


## Ecological Fallacy

What is it and why does it matter?

- The level at which our data are aggregated matters
- Drawing inferences across levels of aggregation can be tricky
- For example, income and voting behavior in US elections
  - Wealthier states tend to break for Democratic candidates
  - Wealthier individuals tend to break for Republican candidates




## Simpson's Paradox

What is it and why does it matter?

- Simpson's Paradox is a special case of confounding

- Simple bivariate relationship between X and Y may appear to show one thing

- But when we adjust for other relevant variables (i.e. Z) the apparent relationship between X and Y reverses

- Common in lots of fields of study




## Simpson's Paradox

::::{.columns}
:::{.column}
Example:

- Imagine we have data on the exercise activity and cholesterol of a few hundred adults

- We plot the individuals' cholesterol levels against their level of exercise (we'll assume we directly observe this and it's not self-reported)

- In the plot we see that exercise appears to correlate positively with cholesterol levels. A simple regression model supports this eyeball estimate.

```{r}
# Set sample size. 1000 is a nice number that doesn't give too much away via
# high density plotting.
N <- 1e4

# Make up some simulated data
# Focus is on the relationship between cholesterol and exercise
# But age is a confounder
data2 <- data.frame(x = rnorm(N, mean = 8, sd = 3)) |>
  mutate(y = 130 + 10 * x + rnorm(N, mean = 0, sd = 10)) |>
  filter(x^2 + y^2 < 42000 & x^2 + y^2 > 35000) |>  # Eliminate some outliers so it looks more rounded with less harsh slopes at edges
  mutate(sum = x + y,
         age = ntile(x = sum,
                     n = 5),
         age = factor(age,
                      labels = c("Age 1", "Age 2", "Age 3", "Age 4", "Age 5")))

lm(y ~ x, data = data2)
```

:::

:::{.column}
```{r simpsons-paradox-1, fig.pos="center", dpi=400}
#| echo: false
#| out.width: "88%"

# Plot the data without grouping to show positive correlation
# between exercise and cholesterol
ggplot(data2, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_continuous(breaks = seq(round(min(data2$x),0), round(max(data2$x), 0), 1),
                     limits = c(round(min(data2$x),0), round(max(data2$x), 0))) +
  theme_simpsons() +
  theme(plot.title = element_text(size = 22)) +
  labs(x = "Exercise",
       y = "Cholesterol",
       title = "Relationship between exercise and cholesterol",
       subtitle = "How could cholesterol increase with more exercise?")

```
:::
::::



## Simpson's Paradox

::::{.columns}
:::{.column}
Example:

- But what if we're omitting some key variables?

- It's possible for results to reverse if we're omitting a variable that could also affect cholesterol levels

- Another way to say this is that the effect of exercise might be **biased** because we're leaving other important things out of the model.

- One good variable would be patient age, so let's see what that looks like

```{r}
lm(y ~ x + age, data = data2)
```

:::

:::{.column}
```{r simpsons-paradox-2, fig.pos="center"}
#| echo: false
#| out.width: "88%"

# Set sample size. 1000 is a nice number that doesn't give too much away via
# high density plotting.

# Plot grouped data to show how exercise correlations negatively once
# we take into account the influence of age.
ggplot(data2, aes(x = x, y = y, color = age)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_color_manual(values = c("hotpink1", "red3", "blue2", "green3", "purple2")) +
  scale_x_continuous(breaks = seq(round(min(data2$x),0), round(max(data2$x), 0), 1),
                     limits = c(round(min(data2$x),0), round(max(data2$x), 0))) +  
  theme_simpsons() +
  theme(plot.title = element_text(size = 22)) +
  labs(x = "Exercise",
       y = "Cholesterol",
       color = "Age",
       title = "Relationship between exercise and cholesterol",
       subtitle = "But what about when we take age into consideration?")

```
:::
::::





## Collider Bias

::::{.columns}
:::{.column}
What is collider bias?

- Collider bias can be introduced by adjusting for a common effect.

- It can also be a form of selection bias.

- Similar to Simpson's Paradox it can make effects appear to be the opposite of what they are, but also depends on the population of interest.

- In this example Z is a collider because both X and Y cause Z.
:::

:::{.column}
```{r collider-example-1}
#| echo: false
#| dpi: 300
#| out.width: "88%"


dag.df <- dagitty(' dag{
                  "x" [exposure]
                  "y" [outcome]
                  
                  "x" [pos="1,1"]
                  "y" [pos="2,1"]
                  "z" [pos="1.5,2"]
                  
                  "x" -> "y"
                  "y" -> "z" <- "x"
}') %>% 
  tidy_dagitty()



ggplot(dag.df, aes(x = x, xend = xend, y = y, yend = yend)) +
  geom_dag_point(aes(color = name), show.legend = FALSE, size = 20) +
  geom_dag_edges(size = 10) +  
  geom_text(aes(label = name), color = "white", size = 10, parse = TRUE) +
  theme_dag() +
  scale_color_viridis(discrete = TRUE, begin = 0.1, end = 0.9) 

```
:::
::::



## Collider Bias

::::{.columns}
:::{.column}
Example: College Admissions

- Let's assume we're interested in looking at students who have been admitted to college to see what the relationship is between their high school extracurricular activities and GPA

- We collect data on college freshman and compare their extracurricular activities in high school with their GPA.

- Surprisingly, we find a negative relationship. 

- That's sad.

:::

:::{.column}
```{r collider-example-plot-1}
#| echo: false
#| dpi: 300
#| out.width: "88%"

N <- 1e4

collider.data <- data.frame(effort = rnorm(N, mean = 0, sd = 1)) |> 
  mutate(gpa = 0 + 0.5*effort + rnorm(N, mean = 0, sd = 1),
         admitted = ifelse(gpa * effort > 2.75 & gpa>0 & effort>0, "Admitted", "Rejected"))


ggplot(collider.data |> filter(admitted == "Admitted"), aes(x = effort, y = gpa)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  theme_flynn(base_family = "oswald") + 
  labs(x = "Extracurriculars",
       y = "GPA",
       title = "Collider Bias",
       subtitle = "Why do students who are more active with extracurriculars have poorer grades?")

```
:::
::::




## Collider Bias

::::{.columns}
:::{.column}
Example: College Admissions

- But wait!

- If we're looking at students who have already been admitted to college we're implicitly conditioning on a collider variable (i.e. acceptance/admission status)

- If we step back and look at the entire population of high school students we find something different

- Students who have better grades and/or better extracurricular activities are more likely to be admitted to college in the first place.

:::

:::{.column}
```{r collider-example-plot-2,  fig.pos="center"}
#| echo: false
#| dpi: 300
#| out.width: "88%"


ggplot(collider.data, aes(x = effort, y = gpa, color = admitted)) +
  geom_point(alpha = 0.5) +
  theme_flynn(base_family = "oswald") + 
  viridis::scale_color_viridis(discrete = TRUE,
                               option = "magma",
                               end = 0.9) +
  labs(x = "Extracurriculars",
       y = "GPA",
       color = "Admission Status",
       title = "Collider Bias",
       subtitle = "The relationship depends on what population we're examining!")

```
:::
::::






# Role of Theory {.inverse}


## The Role of Theory

What is theory?

Let's start with what it's not

- Not a hypothesis
- Not a hunch
- Not a guess
- Just an idea with little or no supporting evidence


## The Role of Theory 

What is theory?

- More accurately, theory is the story we tell about which variables are important and how those variables are related. 

- It both describes and explains how variables affect one another. 

- It also allows us to make predictions about what we should expect to see in various scenarios

> **Theory:** A tentative conjecture about the causes of some phenomenon of interest (Kellstedt and Whitten 2019, 22)

> **Theory:** A logically consistent set of statements that explains a phenomenon of interest (Frieden, Lake, and Schultz 2022, xxix)
